{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82ee655f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-12 08:15:24] {logging_config.py:23} INFO - Logging configured.\n",
      "[2025-05-12 08:15:24] {main_workflow.py:61} INFO - Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "from main_workflow import load_predictor,split_data,SequenceDataPreparer,prepare_dataloaders,load_autoencoder\n",
    "import pandas as pd\n",
    "from data_preparation.collators import pad_collate_fn \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f685785d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import ( # Your config file\n",
    "    RAW_DATA_PATH, NO_MISSINGS_ENCODED_PATH, DIAG_EMBEDDINGS_PATH, DIAG_LABEL_ENCODER_PATH, LABEL_ENCODERS_PATH,\n",
    "    ICD9_HIERARCHY_PATH, ICD9_CHAPTERS_PATH, SPACY_MODEL_NAME, MISSING_VALUES,\n",
    "    DROP_COLUMNS, ONE_HOT_COLUMNS, ORDINAL_MAPPINGS, TREATMENT_COLUMNS,\n",
    "    TREATMENT_MAPPING, LABEL_ENCODING,\n",
    "\n",
    "    LOG_FILE, RANDOM_SEED, PATIENT_ID_COL, TEST_SPLIT_SIZE, VALIDATION_SPLIT_SIZE,\n",
    "    OTHER_EMBEDDING_DIM, HIDDEN_DIM, NUM_RNN_LAYERS, DROPOUT, USE_GRU, USE_ATTENTION,\n",
    "    ATTENTION_DIM, AE_BATCH_SIZE, AE_EPOCHS, PREDICTOR_EPOCHS,\n",
    "    LEARNED_EMB_COLS, FINETUNE_DIAG_EMBEDDINGS, PRECOMPUTED_EMB_COLS,AE_OPTIMIZER,\n",
    "    AE_LEARNING_RATE,AE_WEIGHT_DECAY, AE_SCHEDULER_FACTOR, AE_SCHEDULER_PATIENCE,\n",
    "    AE_EARLY_STOPPING_PATIENCE, PREDICTOR_OPTIMIZER, PREDICTOR_LEARNING_RATE,\n",
    "    MODELS_DIR, PREDICTOR_EARLY_STOPPING_PATIENCE, PREDICTOR_SCHEDULER_FACTOR,\n",
    "    PREDICTOR_SCHEDULER_PATIENCE, PREDICTOR_WEIGHT_DECAY, PREDICTOR_FINETUNE_ENCODER,\n",
    "    SCALER_PATH, ISOLATION_FOREST_PATH, IF_N_ESTIMATORS, IF_CONTAMINATION,\n",
    "    OUTLIER_MODE, VISIT_ERROR_PERCENTILE,\n",
    "    FINAL_ENCODED_DATA_PATH, ENCOUNTER_ID_COL, TARGET_COL, NUMERICAL_FEATURES,\n",
    "    OHE_FEATURES_PREFIX, ICD9_HIERARCHY_PATH, ICD9_CHAPTERS_PATH,\n",
    "    MAX_SEQ_LENGTH,  AE_MODEL_LOAD_PATH, PREDICTOR_MODEL_LOAD_PATH, RESULTS_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bae05dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.read_csv('../data/diabetic_data_no_na_diag.csv', low_memory=False)\n",
    "\n",
    "df_raw_ids = pd.read_csv('../data/diabetic_data.csv', usecols=['encounter_id', 'patient_nbr'])\n",
    "# Ensure indices align before assigning\n",
    "df_final = df_final.reset_index(drop=True)\n",
    "df_raw_ids = df_raw_ids.reset_index(drop=True)\n",
    "df_final['encounter_id'] = df_raw_ids['encounter_id']\n",
    "df_final['patient_nbr'] = df_raw_ids['patient_nbr']\n",
    "\n",
    "df_final.reset_index(drop=True, inplace=True) # Ensure clean index\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94c8b06e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-12 08:15:24] {main_workflow.py:101} INFO - --- Splitting Data (Patient Level - Revised Index Handling) ---\n",
      "[2025-05-12 08:15:24] {main_workflow.py:109} INFO - Total rows before split: 101766\n",
      "[2025-05-12 08:15:24] {main_workflow.py:110} INFO - Total unique patients: 71518\n",
      "[2025-05-12 08:15:24] {main_workflow.py:121} INFO - Test set created: 15108 rows, 10728 patients.\n",
      "[2025-05-12 08:15:24] {main_workflow.py:138} INFO - Train set created: 71395 rows, 50062 patients.\n",
      "[2025-05-12 08:15:24] {main_workflow.py:139} INFO - Validation set created: 15263 rows, 10728 patients.\n",
      "[2025-05-12 08:15:24] {main_workflow.py:140} INFO - --- Data Splitting Complete ---\n"
     ]
    }
   ],
   "source": [
    "df_train, df_val, df_test = split_data(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5be7f9cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-12 08:15:27] {sequence_preparer.py:48} INFO - SequenceDataPreparer initialized. Max length: 50\n",
      "[2025-05-12 08:15:27] {sequence_preparer.py:116} INFO - Scaler loaded from c:\\Users\\lukag\\OneDrive\\Desktop\\Universidad\\3ero\\cuadrimestre2\\PAID\\github\\IDSS-for-Diabetes-Readmission-Prediction\\models\\scaler.pkl\n",
      "[2025-05-12 08:15:27] {main_workflow.py:160} INFO - --- Preparing Sequences and DataLoaders ---\n",
      "[2025-05-12 08:15:27] {sequence_preparer.py:84} WARNING - Scaler already fitted or loaded. Skipping fit.\n",
      "[2025-05-12 08:15:27] {sequence_preparer.py:60} INFO - Identified 17 OHE columns.\n",
      "[2025-05-12 08:15:27] {sequence_preparer.py:141} INFO - Transforming DataFrame (71395 rows) into sequences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lukag\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.6.1 when using version 1.5.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-12 08:15:46] {sequence_preparer.py:199} INFO - Created 50062 sequences for 50062 patients.\n",
      "[2025-05-12 08:15:46] {sequence_preparer.py:141} INFO - Transforming DataFrame (15263 rows) into sequences.\n",
      "[2025-05-12 08:15:51] {sequence_preparer.py:199} INFO - Created 10728 sequences for 10728 patients.\n",
      "[2025-05-12 08:15:51] {main_workflow.py:176} INFO - Train and Validation DataLoaders created.\n",
      "[2025-05-12 08:15:51] {main_workflow.py:177} INFO - --- Sequence Preparation Complete ---\n"
     ]
    }
   ],
   "source": [
    "# 4. Prepare DataLoaders\n",
    "data_preparer = SequenceDataPreparer(\n",
    "    patient_id_col=PATIENT_ID_COL, timestamp_col=ENCOUNTER_ID_COL, target_col=TARGET_COL,\n",
    "    numerical_features=NUMERICAL_FEATURES, ohe_feature_prefixes=OHE_FEATURES_PREFIX,\n",
    "    learned_emb_cols=LEARNED_EMB_COLS, precomputed_emb_cols=PRECOMPUTED_EMB_COLS,\n",
    "    max_seq_length=MAX_SEQ_LENGTH, scaler_path=SCALER_PATH\n",
    ")\n",
    "# Need a sample batch to determine dims for loading AE if not training\n",
    "# Prepare loaders *before* deciding whether to train or load AE\n",
    "train_loader, val_loader = prepare_dataloaders(data_preparer, df_train, df_val, AE_BATCH_SIZE)\n",
    "sample_batch_for_build = next(iter(train_loader)) # Get a sample batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39f6b05e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-12 08:16:38] {main_workflow.py:202} INFO - --- Loading Pre-trained Autoencoder from c:\\Users\\lukag\\OneDrive\\Desktop\\Universidad\\3ero\\cuadrimestre2\\PAID\\github\\IDSS-for-Diabetes-Readmission-Prediction\\models\\autoencoder_best.pth ---\n",
      "[2025-05-12 08:16:38] {model_builder.py:10} INFO - Building AE model architecture from config...\n",
      "[2025-05-12 08:16:38] {embeddings.py:32} INFO - Initialized learned embedding for 'discharge_disposition_id' (Vocab: 26, Dim: 10)\n",
      "[2025-05-12 08:16:38] {embeddings.py:32} INFO - Initialized learned embedding for 'admission_source_id' (Vocab: 17, Dim: 10)\n",
      "[2025-05-12 08:16:38] {embeddings.py:45} INFO - Initialized precomputed embedding for 'diag_1' (Shape: torch.Size([916, 8]), Finetune: True)\n",
      "[2025-05-12 08:16:38] {embeddings.py:45} INFO - Initialized precomputed embedding for 'diag_2' (Shape: torch.Size([916, 8]), Finetune: True)\n",
      "[2025-05-12 08:16:38] {embeddings.py:45} INFO - Initialized precomputed embedding for 'diag_3' (Shape: torch.Size([916, 8]), Finetune: True)\n",
      "[2025-05-12 08:16:38] {embeddings.py:47} INFO - Total Learned Embedding Dim: 20\n",
      "[2025-05-12 08:16:38] {embeddings.py:48} INFO - Total Precomputed Embedding Dim: 24\n",
      "[2025-05-12 08:16:38] {model_builder.py:23} INFO - Determined Num OHE Features for build: 26\n",
      "[2025-05-12 08:16:38] {model_builder.py:37} INFO - AE model architecture built.\n",
      "[2025-05-12 08:16:38] {helpers.py:41} INFO - PyTorch artifact loaded from c:\\Users\\lukag\\OneDrive\\Desktop\\Universidad\\3ero\\cuadrimestre2\\PAID\\github\\IDSS-for-Diabetes-Readmission-Prediction\\models\\autoencoder_best.pth to device 'cpu'.\n",
      "[2025-05-12 08:16:38] {main_workflow.py:219} INFO - Pre-trained Autoencoder loaded successfully.\n",
      "[2025-05-12 08:16:38] {main_workflow.py:279} INFO - --- Loading Pre-trained PredictorModel from c:\\Users\\lukag\\OneDrive\\Desktop\\Universidad\\3ero\\cuadrimestre2\\PAID\\github\\IDSS-for-Diabetes-Readmission-Prediction\\models\\predictor_best.pth ---\n",
      "[2025-05-12 08:16:38] {model_builder.py:10} INFO - Building AE model architecture from config...\n",
      "[2025-05-12 08:16:38] {embeddings.py:32} INFO - Initialized learned embedding for 'discharge_disposition_id' (Vocab: 26, Dim: 10)\n",
      "[2025-05-12 08:16:38] {embeddings.py:32} INFO - Initialized learned embedding for 'admission_source_id' (Vocab: 17, Dim: 10)\n",
      "[2025-05-12 08:16:38] {embeddings.py:45} INFO - Initialized precomputed embedding for 'diag_1' (Shape: torch.Size([916, 8]), Finetune: True)\n",
      "[2025-05-12 08:16:38] {embeddings.py:45} INFO - Initialized precomputed embedding for 'diag_2' (Shape: torch.Size([916, 8]), Finetune: True)\n",
      "[2025-05-12 08:16:38] {embeddings.py:45} INFO - Initialized precomputed embedding for 'diag_3' (Shape: torch.Size([916, 8]), Finetune: True)\n",
      "[2025-05-12 08:16:38] {embeddings.py:47} INFO - Total Learned Embedding Dim: 20\n",
      "[2025-05-12 08:16:38] {embeddings.py:48} INFO - Total Precomputed Embedding Dim: 24\n",
      "[2025-05-12 08:16:38] {model_builder.py:23} INFO - Determined Num OHE Features for build: 26\n",
      "[2025-05-12 08:16:38] {model_builder.py:37} INFO - AE model architecture built.\n",
      "[2025-05-12 08:16:38] {helpers.py:41} INFO - PyTorch artifact loaded from c:\\Users\\lukag\\OneDrive\\Desktop\\Universidad\\3ero\\cuadrimestre2\\PAID\\github\\IDSS-for-Diabetes-Readmission-Prediction\\models\\predictor_best.pth to device 'cpu'.\n",
      "[2025-05-12 08:16:38] {main_workflow.py:303} INFO - Pre-trained PredictorModel loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lukag\\OneDrive\\Desktop\\Universidad\\3ero\\cuadrimestre2\\PAID\\github\\IDSS-for-Diabetes-Readmission-Prediction\\src\\utils\\helpers.py:40: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  artifact = torch.load(path, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "ae_model_load_path = AE_MODEL_LOAD_PATH # Load path from config\n",
    "trained_ae = load_autoencoder(ae_model_load_path, sample_batch_for_build)\n",
    "\n",
    "predictor_model_load_path = PREDICTOR_MODEL_LOAD_PATH # Load path from config\n",
    "trained_predictor = load_predictor(predictor_model_load_path, sample_batch_for_build) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77875c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-12 08:29:37] {sequence_preparer.py:141} INFO - Transforming DataFrame (71395 rows) into sequences.\n",
      "[2025-05-12 08:29:57] {sequence_preparer.py:199} INFO - Created 50062 sequences for 50062 patients.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m feature_seqs, _, _ \u001b[38;5;241m=\u001b[39m data_preparer\u001b[38;5;241m.\u001b[39mtransform(df_train)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# 2. Aplica pad_collate_fn para obtener un batch\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m batch_bg \u001b[38;5;241m=\u001b[39m \u001b[43mpad_collate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature_seqs\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# 3. Extrae únicamente los features y la máscara\u001b[39;00m\n\u001b[0;32m      6\u001b[0m X_bg \u001b[38;5;241m=\u001b[39m batch_bg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mnumpy()       \u001b[38;5;66;03m# shape (50, seq_len, feat_dim)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lukag\\OneDrive\\Desktop\\Universidad\\3ero\\cuadrimestre2\\PAID\\github\\IDSS-for-Diabetes-Readmission-Prediction\\src\\data_preparation\\collators.py:32\u001b[0m, in \u001b[0;36mpad_collate_fn\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124;03mCollate function to pad sequences within a batch and restructure features.\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;124;03m    }\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Determine max sequence length in this batch\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m lengths \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[43m[\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlength\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m]\u001b[49m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[0;32m     33\u001b[0m max_len \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(lengths)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# --- Initialize lists to hold padded data for each feature type ---\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Numerical + OHE\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lukag\\OneDrive\\Desktop\\Universidad\\3ero\\cuadrimestre2\\PAID\\github\\IDSS-for-Diabetes-Readmission-Prediction\\src\\data_preparation\\collators.py:32\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124;03mCollate function to pad sequences within a batch and restructure features.\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;124;03m    }\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Determine max sequence length in this batch\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m lengths \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[43mitem\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlength\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m batch], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[0;32m     33\u001b[0m max_len \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(lengths)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# --- Initialize lists to hold padded data for each feature type ---\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Numerical + OHE\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "# 1. Toma 50–100 secuencias de entrenamiento\n",
    "feature_seqs, _, _ = data_preparer.transform(df_train)\n",
    "# 2. Aplica pad_collate_fn para obtener un batch\n",
    "batch_bg = pad_collate_fn(feature_seqs[:50])  \n",
    "# 3. Extrae únicamente los features y la máscara\n",
    "X_bg = batch_bg['features'].numpy()       # shape (50, seq_len, feat_dim)\n",
    "mask_bg = batch_bg['mask'].numpy().astype(bool)  # shape (50, seq_len)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
