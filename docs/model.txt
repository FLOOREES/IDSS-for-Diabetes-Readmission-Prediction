1. Introduction

This document outlines a proposed architecture and methodology for analyzing a sequential dataset of diabetes patient encounters derived from the Health Facts database. The data represents multiple hospital visits per patient over time, exhibiting inherent temporal dependencies and a non-IID (Independent and Identically Distributed) nature. Our primary goals are twofold: robust outlier detection to identify unusual patient trajectories or visits, and accurate prediction of hospital readmission (readmitted status) for each encounter based on the patient's history up to that point. This proposal details a unified deep learning approach using sequence models to effectively handle these challenges while promoting component reusability.

2. Objectives

Develop a data preparation pipeline specifically designed for sequential clinical encounter data, handling feature encoding, scaling, padding, and masking.

Implement and train a sequence Autoencoder (AE), likely using LSTM or GRU with Attention, to learn meaningful, compressed representations (embeddings) of patient encounter histories.

Establish methods for outlier detection using the trained AE, supporting both:

Visit-level anomaly identification based on reconstruction error.

Patient-level anomaly identification based on learned patient embeddings.

Develop a sequence prediction model, leveraging the pre-trained AE encoder, to predict the readmitted status for each encounter in a patient's history in a causally consistent manner (using only past and current information).

Define clear evaluation strategies and metrics for both outlier detection performance and prediction accuracy.

Provide a robust rationale for choosing sequence models over traditional tabular methods like LightGBM for this specific problem context.

3. Proposed Solution: Two-Stage Sequence Modeling Pipeline

We propose a pipeline centered around a recurrent neural network (RNN), specifically LSTM or GRU, augmented with an Attention mechanism, implemented in PyTorch. The pipeline consists of two main stages:

Stage 1: Autoencoder Pre-training for Representation Learning & Outlier Detection: Train a sequence-to-sequence Autoencoder (Encoder-Decoder architecture). The Encoder learns to compress variable-length visit sequences into context-rich representations. The Decoder attempts to reconstruct the original sequence from these representations. This stage provides the foundation for both outlier detection and the subsequent prediction task.

Stage 2: Prediction Model Fine-tuning/Training: Reuse the trained Encoder from Stage 1. Attach a dedicated Prediction Head (typically a linear layer with appropriate activation) to the Encoder's output at each timestep. Train this combined model (Encoder + Head) specifically for the readmitted prediction task, fine-tuning the encoder weights as needed.

4. Methodology & Implementation Details

4.1. Data Preparation Module:

Input: DataFrame after initial preprocessing (e.g., df_phase2 containing scaled numerical features, OHE features, and label-encoded categoricals like diagnoses).

Steps:

Grouping & Sorting: Group data by patient_nbr. Sort encounters within each group chronologically using encounter_id.

Feature Selection: Define the feature set for the sequence model. Exclude identifiers (encounter_id, patient_nbr) and the target (readmitted) from the input features for the current timestep.

Feature Vector Construction (per visit):

Identify numerical/OHE features (already scaled/encoded).

Identify label-encoded features requiring learned embeddings (e.g., admission_source_id if not pre-computed).

Identify label-encoded features using pre-computed embeddings (diag_1, diag_2, diag_3).

Implementation: This step will involve creating lists/dictionaries mapping column names to their type (numerical, OHE, learned-embedding, precomputed-embedding) to guide model building.

Sequence Generation: For each patient, create a list of feature vectors, one vector per visit. Store these sequences along with the corresponding sequence of readmitted labels.

Padding: Pad all sequences (features and labels) to a fixed max_seq_length using a consistent padding value (e.g., 0). Pre-padding is typical. max_seq_length should be chosen based on visit distribution analysis (e.g., 95th percentile length).

Masking: Generate a boolean mask tensor (True for real data, False for padding) with shape (batch_size, max_seq_length).

PyTorch Dataset & DataLoader: Implement a custom Dataset to hold sequences, labels, and masks. Use a DataLoader with a collate_fn to handle batching and ensure consistent padding/masking within each batch.

Train/Validation Split: Perform a patient-level split before sequence generation using GroupShuffleSplit.

4.2. Model Architecture Module (PyTorch nn.Module)

Input Processing:

Receive batches of padded sequences and masks.

Embedding Layers:

nn.Embedding layers for features needing learned embeddings. Input size = vocab size for that feature, output size = desired embedding dim.

nn.Embedding for pre-computed diagnosis embeddings. Input size = number of unique diagnosis codes, output size = pre-computed embedding dim. Load weights from diag_embeddings.npy using torch.from_numpy(). Set embedding_layer.weight.requires_grad = False (freeze) or True (fine-tune). Use the integer labels (diag_1, diag_2, diag_3) as indices.

Concatenation: Concatenate numerical features, OHE features, learned embedding outputs, and pre-computed embedding lookups for each timestep into a single input vector for the RNN.

Encoder:

Input: Concatenated feature sequences.

Core: nn.LSTM or nn.GRU layer (batch_first=True, return_sequences=True). Consider multiple layers.

Attention (Strongly Recommended): Implement an attention mechanism (e.g., Additive/Bahdanau or Multiplicative/Luong) that allows subsequent layers to weigh the importance of different timesteps from the encoder's output sequence (encoder_outputs).

Output: encoder_outputs (shape: batch, seq_len, hidden_dim) and encoder_final_hidden.

Decoder (for AE Pre-training):

Input: encoder_outputs and/or encoder_final_hidden.

Core: nn.LSTM or nn.GRU layer. Can optionally use attention to query encoder_outputs.

Output Layer: nn.Linear projecting the decoder's hidden state at each timestep back to the dimension of the original concatenated input feature vector.

Prediction Head (for Prediction Task):

Input: encoder_outputs (shape: batch, seq_len, hidden_dim).

Core: nn.Linear(hidden_dim, num_readmission_classes) followed by nn.LogSoftmax(dim=-1) or nn.Sigmoid() depending on binary/multi-class output.

Output: Logits or probabilities for each timestep (shape: batch, seq_len, num_classes).

4.3. Training Strategy Module:

AE Pre-training:

Instantiate Encoder and Decoder.

Use Adam or similar optimizer.

Train using Masked Reconstruction Loss (e.g., MSE or MAE). Calculate loss element-wise, multiply by the expanded mask (mask.unsqueeze(-1)), sum/average over non-masked elements.

Use learning rate scheduling and early stopping based on validation reconstruction loss.

Save the trained Encoder weights.

Prediction Model Training/Fine-tuning:

Instantiate the trained Encoder and the Prediction Head.

Load the saved Encoder weights. Decide on freezing (requires_grad=False) vs. fine-tuning (lower learning rate for encoder layers initially).

Use Adam or similar optimizer.

Train using Masked Prediction Loss (e.g., nn.NLLLoss if using LogSoftmax, nn.BCELoss if using Sigmoid). Apply the mask to ignore losses on padded timesteps. Calculate loss per valid timestep and average.

Use learning rate scheduling and early stopping based on validation prediction loss (or target metric like AUC).

Save the combined trained model (Encoder + Head).

4.4. Outlier Detection Module:

Implementation: Requires the trained Autoencoder (Encoder + Decoder).

Visit-Level Mode:

Load trained AE model. Set to eval() mode.

Prepare input sequence (padding, mask).

Pass sequence through AE to get reconstructions.

Calculate per-timestep reconstruction error (e.g., MAE summed across features) between input and reconstruction.

Apply mask to get errors only for valid timesteps.

Determine threshold (e.g., 95th percentile) based on valid errors from the training dataset.

Flag visits in new data whose masked error exceeds the threshold.

Patient-Level Mode:

Load trained Encoder. Set to eval() mode.

Prepare input sequences.

Pass sequences through Encoder, extract encoder_final_hidden state (or pool encoder_outputs) as the patient embedding.

Train sklearn.ensemble.IsolationForest on the embeddings from the training dataset.

Use the trained Isolation Forest's decision_function or predict method on embeddings from new data to get anomaly scores or flags.

4.5. Prediction Module:

Implementation: Requires the trained Encoder + Prediction Head model.

Process:

Load trained model. Set to eval() mode.

Prepare input sequence [v1, ..., vN] (padding, mask).

Pass sequence through the model. Output shape: (1, seq_len, num_classes).

The output at index t (i.e., output[:, t, :]) represents the prediction for visit v(t+1) based on history v1...v(t+1).

Extract the prediction for the desired timestep(s). For predicting visit N's outcome using history v1..vN, you use the model output at the N-1 index (or the last valid index according to the mask if padded).

5. Evaluation

Autoencoder: Average Masked Reconstruction Loss (MSE/MAE) on validation/test sets. Qualitative analysis of reconstructions. Visualization of patient embeddings (t-SNE/UMAP).

Outlier Detection: If labels exist: Precision, Recall, F1-score for flagged outliers. If no labels: Analysis of score distributions, qualitative examination of flagged outliers.

Prediction: Standard classification metrics applied timestep-wise on validation/test sets (masked): AUC-ROC, AUC-PR, F1-Score, Accuracy, Confusion Matrix. Evaluate performance specifically for single-visit vs. multi-visit patients.

6. Rationale vs. Tabular Methods (e.g., LightGBM)

Using a sequence model (LSTM/GRU+Attention) is fundamentally more appropriate for this data than directly applying LightGBM to flattened visits or even patient-level embeddings for step-by-step visit prediction:

Handles Temporal Dependencies: RNNs are explicitly designed to capture ordered relationships between visits, which LightGBM ignores.

Automatic Feature Engineering: The RNN learns relevant historical features automatically, reducing the need for manual creation of features like "time since last visit" or "change in diagnosis count".

Sequence-to-Sequence Native: The proposed architecture naturally handles predicting an outcome for each step in the sequence, aligning perfectly with the goal of predicting readmission for every visit based on its history. LightGBM requires awkward feature engineering or restructuring to approximate this.

Parameter Efficiency & Shared Embeddings: Embeddings (learned or pre-trained) are integrated seamlessly and shared across timesteps and potentially across tasks (AE and prediction).

Causal Consistency: The sequence model inherently processes data chronologically, preventing information leakage from future visits into past predictions.

While using patient embeddings from the encoder as input features to LightGBM is valid for patient-level prediction or outlier detection, it is suboptimal for predicting the outcome of each individual visit within its historical context.

7. Deliverables

Documented Python code for Data Preparation, Model Architecture, Training, Outlier Detection, and Prediction modules.

Saved model weights for the trained Autoencoder and Prediction Model.

Saved outlier detection model (e.g., Isolation Forest state) if applicable.

Scripts for training, evaluation, and inference.

Performance reports detailing reconstruction error, outlier detection results, and prediction metrics.

8. Potential Challenges & Mitigation

Data Sparsity/Quality: Address via robust preprocessing and potentially masking specific unreliable features.

Hyperparameter Tuning: Requires careful tuning (learning rate, hidden dims, layers, dropout, max_seq_length). Use validation sets and potentially tools like Optuna/Ray Tune.

Computational Resources: Training deep sequence models can be intensive. Utilize GPU resources and efficient data loading.

Interpretability: RNNs and embeddings can be black boxes. Use attention weight visualization, SHAP (for model outputs), or embedding analysis techniques.

Single-Visit Patients: The model must handle sequences of length 1 gracefully (padding helps technically). Evaluate if performance differs significantly for these patients.

9. Conclusion

This two-stage sequential modeling approach provides a principled, powerful, and flexible framework for analyzing the diabetes encounter data. It directly addresses the temporal dependencies, enables effective outlier detection, facilitates accurate, causally consistent readmission prediction, and allows for valuable component reuse, representing a state-of-the-art methodology for this type of clinical data.