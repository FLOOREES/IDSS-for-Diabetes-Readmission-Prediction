{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82ee655f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from main_workflow import load_predictor,split_data,SequenceDataPreparer,prepare_dataloaders,load_autoencoder\n",
    "import pandas as pd\n",
    "from data_preparation.collators import pad_collate_fn \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f685785d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import ( # Your config file\n",
    "    RAW_DATA_PATH, NO_MISSINGS_ENCODED_PATH, DIAG_EMBEDDINGS_PATH, DIAG_LABEL_ENCODER_PATH, LABEL_ENCODERS_PATH,\n",
    "    ICD9_HIERARCHY_PATH, ICD9_CHAPTERS_PATH, SPACY_MODEL_NAME, MISSING_VALUES,\n",
    "    DROP_COLUMNS, ONE_HOT_COLUMNS, ORDINAL_MAPPINGS, TREATMENT_COLUMNS,\n",
    "    TREATMENT_MAPPING, LABEL_ENCODING,\n",
    "\n",
    "    LOG_FILE, RANDOM_SEED, PATIENT_ID_COL, TEST_SPLIT_SIZE, VALIDATION_SPLIT_SIZE,\n",
    "    OTHER_EMBEDDING_DIM, HIDDEN_DIM, NUM_RNN_LAYERS, DROPOUT, USE_GRU, USE_ATTENTION,\n",
    "    ATTENTION_DIM, AE_BATCH_SIZE, AE_EPOCHS, PREDICTOR_EPOCHS,\n",
    "    LEARNED_EMB_COLS, FINETUNE_DIAG_EMBEDDINGS, PRECOMPUTED_EMB_COLS,AE_OPTIMIZER,\n",
    "    AE_LEARNING_RATE,AE_WEIGHT_DECAY, AE_SCHEDULER_FACTOR, AE_SCHEDULER_PATIENCE,\n",
    "    AE_EARLY_STOPPING_PATIENCE, PREDICTOR_OPTIMIZER, PREDICTOR_LEARNING_RATE,\n",
    "    MODELS_DIR, PREDICTOR_EARLY_STOPPING_PATIENCE, PREDICTOR_SCHEDULER_FACTOR,\n",
    "    PREDICTOR_SCHEDULER_PATIENCE, PREDICTOR_WEIGHT_DECAY, PREDICTOR_FINETUNE_ENCODER,\n",
    "    SCALER_PATH, ISOLATION_FOREST_PATH, IF_N_ESTIMATORS, IF_CONTAMINATION,\n",
    "    OUTLIER_MODE, VISIT_ERROR_PERCENTILE,\n",
    "    FINAL_ENCODED_DATA_PATH, ENCOUNTER_ID_COL, TARGET_COL, NUMERICAL_FEATURES,\n",
    "    OHE_FEATURES_PREFIX, ICD9_HIERARCHY_PATH, ICD9_CHAPTERS_PATH,\n",
    "    MAX_SEQ_LENGTH,  AE_MODEL_LOAD_PATH, PREDICTOR_MODEL_LOAD_PATH, RESULTS_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bae05dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.read_csv('../data/diabetic_data_no_na_diag.csv', low_memory=False)\n",
    "\n",
    "df_raw_ids = pd.read_csv('../data/diabetic_data.csv', usecols=['encounter_id', 'patient_nbr'])\n",
    "# Ensure indices align before assigning\n",
    "df_final = df_final.reset_index(drop=True)\n",
    "df_raw_ids = df_raw_ids.reset_index(drop=True)\n",
    "df_final['encounter_id'] = df_raw_ids['encounter_id']\n",
    "df_final['patient_nbr'] = df_raw_ids['patient_nbr']\n",
    "\n",
    "df_final.reset_index(drop=True, inplace=True) # Ensure clean index\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94c8b06e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-30 08:46:59] {main_workflow.py:101} INFO - --- Splitting Data (Patient Level - Revised Index Handling) ---\n",
      "[2025-04-30 08:46:59] {main_workflow.py:109} INFO - Total rows before split: 101766\n",
      "[2025-04-30 08:46:59] {main_workflow.py:110} INFO - Total unique patients: 71518\n",
      "[2025-04-30 08:46:59] {main_workflow.py:121} INFO - Test set created: 15108 rows, 10728 patients.\n",
      "[2025-04-30 08:46:59] {main_workflow.py:138} INFO - Train set created: 71395 rows, 50062 patients.\n",
      "[2025-04-30 08:46:59] {main_workflow.py:139} INFO - Validation set created: 15263 rows, 10728 patients.\n",
      "[2025-04-30 08:46:59] {main_workflow.py:140} INFO - --- Data Splitting Complete ---\n"
     ]
    }
   ],
   "source": [
    "df_train, df_val, df_test = split_data(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5be7f9cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-30 08:46:59] {sequence_preparer.py:48} INFO - SequenceDataPreparer initialized. Max length: 50\n",
      "[2025-04-30 08:46:59] {sequence_preparer.py:116} INFO - Scaler loaded from c:\\Users\\lukag\\OneDrive\\Desktop\\Universidad\\3ero\\cuadrimestre2\\PAID\\github\\IDSS-for-Diabetes-Readmission-Prediction\\models\\scaler.pkl\n",
      "[2025-04-30 08:46:59] {main_workflow.py:160} INFO - --- Preparing Sequences and DataLoaders ---\n",
      "[2025-04-30 08:46:59] {sequence_preparer.py:84} WARNING - Scaler already fitted or loaded. Skipping fit.\n",
      "[2025-04-30 08:46:59] {sequence_preparer.py:60} INFO - Identified 17 OHE columns.\n",
      "[2025-04-30 08:46:59] {sequence_preparer.py:141} INFO - Transforming DataFrame (71395 rows) into sequences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lukag\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.6.1 when using version 1.5.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-30 08:47:38] {sequence_preparer.py:199} INFO - Created 50062 sequences for 50062 patients.\n",
      "[2025-04-30 08:47:38] {sequence_preparer.py:141} INFO - Transforming DataFrame (15263 rows) into sequences.\n",
      "[2025-04-30 08:47:46] {sequence_preparer.py:199} INFO - Created 10728 sequences for 10728 patients.\n",
      "[2025-04-30 08:47:46] {main_workflow.py:176} INFO - Train and Validation DataLoaders created.\n",
      "[2025-04-30 08:47:46] {main_workflow.py:177} INFO - --- Sequence Preparation Complete ---\n"
     ]
    }
   ],
   "source": [
    "# 4. Prepare DataLoaders\n",
    "data_preparer = SequenceDataPreparer(\n",
    "    patient_id_col=PATIENT_ID_COL, timestamp_col=ENCOUNTER_ID_COL, target_col=TARGET_COL,\n",
    "    numerical_features=NUMERICAL_FEATURES, ohe_feature_prefixes=OHE_FEATURES_PREFIX,\n",
    "    learned_emb_cols=LEARNED_EMB_COLS, precomputed_emb_cols=PRECOMPUTED_EMB_COLS,\n",
    "    max_seq_length=MAX_SEQ_LENGTH, scaler_path=SCALER_PATH\n",
    ")\n",
    "# Need a sample batch to determine dims for loading AE if not training\n",
    "# Prepare loaders *before* deciding whether to train or load AE\n",
    "train_loader, val_loader = prepare_dataloaders(data_preparer, df_train, df_val, AE_BATCH_SIZE)\n",
    "sample_batch_for_build = next(iter(train_loader)) # Get a sample batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39f6b05e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-30 08:48:47] {main_workflow.py:202} INFO - --- Loading Pre-trained Autoencoder from c:\\Users\\lukag\\OneDrive\\Desktop\\Universidad\\3ero\\cuadrimestre2\\PAID\\github\\IDSS-for-Diabetes-Readmission-Prediction\\models\\autoencoder_best.pth ---\n",
      "[2025-04-30 08:48:47] {model_builder.py:10} INFO - Building AE model architecture from config...\n",
      "[2025-04-30 08:48:47] {embeddings.py:32} INFO - Initialized learned embedding for 'discharge_disposition_id' (Vocab: 26, Dim: 10)\n",
      "[2025-04-30 08:48:47] {embeddings.py:32} INFO - Initialized learned embedding for 'admission_source_id' (Vocab: 17, Dim: 10)\n",
      "[2025-04-30 08:48:47] {embeddings.py:45} INFO - Initialized precomputed embedding for 'diag_1' (Shape: torch.Size([916, 8]), Finetune: True)\n",
      "[2025-04-30 08:48:47] {embeddings.py:45} INFO - Initialized precomputed embedding for 'diag_2' (Shape: torch.Size([916, 8]), Finetune: True)\n",
      "[2025-04-30 08:48:47] {embeddings.py:45} INFO - Initialized precomputed embedding for 'diag_3' (Shape: torch.Size([916, 8]), Finetune: True)\n",
      "[2025-04-30 08:48:47] {embeddings.py:47} INFO - Total Learned Embedding Dim: 20\n",
      "[2025-04-30 08:48:47] {embeddings.py:48} INFO - Total Precomputed Embedding Dim: 24\n",
      "[2025-04-30 08:48:47] {model_builder.py:23} INFO - Determined Num OHE Features for build: 26\n",
      "[2025-04-30 08:48:47] {model_builder.py:37} INFO - AE model architecture built.\n",
      "[2025-04-30 08:48:47] {helpers.py:41} INFO - PyTorch artifact loaded from c:\\Users\\lukag\\OneDrive\\Desktop\\Universidad\\3ero\\cuadrimestre2\\PAID\\github\\IDSS-for-Diabetes-Readmission-Prediction\\models\\autoencoder_best.pth to device 'cpu'.\n",
      "[2025-04-30 08:48:47] {main_workflow.py:219} INFO - Pre-trained Autoencoder loaded successfully.\n",
      "[2025-04-30 08:48:47] {main_workflow.py:279} INFO - --- Loading Pre-trained PredictorModel from c:\\Users\\lukag\\OneDrive\\Desktop\\Universidad\\3ero\\cuadrimestre2\\PAID\\github\\IDSS-for-Diabetes-Readmission-Prediction\\models\\predictor_best.pth ---\n",
      "[2025-04-30 08:48:47] {model_builder.py:10} INFO - Building AE model architecture from config...\n",
      "[2025-04-30 08:48:47] {embeddings.py:32} INFO - Initialized learned embedding for 'discharge_disposition_id' (Vocab: 26, Dim: 10)\n",
      "[2025-04-30 08:48:47] {embeddings.py:32} INFO - Initialized learned embedding for 'admission_source_id' (Vocab: 17, Dim: 10)\n",
      "[2025-04-30 08:48:47] {embeddings.py:45} INFO - Initialized precomputed embedding for 'diag_1' (Shape: torch.Size([916, 8]), Finetune: True)\n",
      "[2025-04-30 08:48:47] {embeddings.py:45} INFO - Initialized precomputed embedding for 'diag_2' (Shape: torch.Size([916, 8]), Finetune: True)\n",
      "[2025-04-30 08:48:47] {embeddings.py:45} INFO - Initialized precomputed embedding for 'diag_3' (Shape: torch.Size([916, 8]), Finetune: True)\n",
      "[2025-04-30 08:48:47] {embeddings.py:47} INFO - Total Learned Embedding Dim: 20\n",
      "[2025-04-30 08:48:47] {embeddings.py:48} INFO - Total Precomputed Embedding Dim: 24\n",
      "[2025-04-30 08:48:47] {model_builder.py:23} INFO - Determined Num OHE Features for build: 26\n",
      "[2025-04-30 08:48:47] {model_builder.py:37} INFO - AE model architecture built.\n",
      "[2025-04-30 08:48:47] {helpers.py:41} INFO - PyTorch artifact loaded from c:\\Users\\lukag\\OneDrive\\Desktop\\Universidad\\3ero\\cuadrimestre2\\PAID\\github\\IDSS-for-Diabetes-Readmission-Prediction\\models\\predictor_best.pth to device 'cpu'.\n",
      "[2025-04-30 08:48:47] {main_workflow.py:303} INFO - Pre-trained PredictorModel loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lukag\\OneDrive\\Desktop\\Universidad\\3ero\\cuadrimestre2\\PAID\\github\\IDSS-for-Diabetes-Readmission-Prediction\\src\\utils\\helpers.py:40: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  artifact = torch.load(path, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "ae_model_load_path = AE_MODEL_LOAD_PATH # Load path from config\n",
    "trained_ae = load_autoencoder(ae_model_load_path, sample_batch_for_build)\n",
    "\n",
    "predictor_model_load_path = PREDICTOR_MODEL_LOAD_PATH # Load path from config\n",
    "trained_predictor = load_predictor(predictor_model_load_path, sample_batch_for_build) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "806b6c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from analysis.predictor_inference import Predictor\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9eb47431",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Dict\n",
    "class ShapModel(nn.Module):\n",
    "    def __init__(self, predictor_model):\n",
    "        super().__init__()\n",
    "        self.pred = predictor_model.eval()\n",
    "\n",
    "    def forward(self, batch: Dict[str, torch.Tensor]) -> torch.Tensor:\n",
    "        # batch ya viene de pad_collate_fn:\n",
    "        #   batch['features'] → el dict interno\n",
    "        #   batch['mask']     → Tensor(batch, seq_len)\n",
    "        logits = self.pred({\n",
    "            'features': batch['features'],\n",
    "            'mask': batch['mask']\n",
    "        })                                     # → (batch, seq_len, n_classes)\n",
    "        probs  = torch.softmax(logits, dim=-1)  # → (batch, seq_len, n_classes)\n",
    "        return probs[:, -1, :]                  # → (batch, n_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e7988ef1",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 61\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcollate_bg\u001b[39m(list_of_dicts):\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;66;03m# reutiliza tu pad_collate_fn desplegando\u001b[39;00m\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;66;03m#   cada dict como si fuese un batch de tamaño 1\u001b[39;00m\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pad_collate_fn([\n\u001b[0;32m     48\u001b[0m       {\n\u001b[0;32m     49\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m: {\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     58\u001b[0m       \u001b[38;5;28;01mfor\u001b[39;00m ds \u001b[38;5;129;01min\u001b[39;00m list_of_dicts\n\u001b[0;32m     59\u001b[0m     ])\n\u001b[1;32m---> 61\u001b[0m bg_collated \u001b[38;5;241m=\u001b[39m \u001b[43mcollate_bg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbg_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# bg_collated['features'] es el dict con tensores (50, seq_len, …)\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# bg_collated['mask']     es (50, seq_len)\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# Pásalo a device\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k,v \u001b[38;5;129;01min\u001b[39;00m bg_collated[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mitems():\n",
      "Cell \u001b[1;32mIn[18], line 47\u001b[0m, in \u001b[0;36mcollate_bg\u001b[1;34m(list_of_dicts)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcollate_bg\u001b[39m(list_of_dicts):\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;66;03m# reutiliza tu pad_collate_fn desplegando\u001b[39;00m\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;66;03m#   cada dict como si fuese un batch de tamaño 1\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpad_collate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m      \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfeatures\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[43m          \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnum_ohe\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mds\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfeatures\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnum_ohe\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[43m          \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlearned_labels\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mds\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfeatures\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlearned_labels\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mds\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfeatures\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlearned_labels\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[43m          \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprecomputed_labels\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mds\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfeatures\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprecomputed_labels\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mds\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfeatures\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprecomputed_labels\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtargets\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# dummy\u001b[39;49;00m\n\u001b[0;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlength\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mds\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpatient_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[0;32m     57\u001b[0m \u001b[43m      \u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mds\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlist_of_dicts\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lukag\\OneDrive\\Desktop\\Universidad\\3ero\\cuadrimestre2\\PAID\\github\\IDSS-for-Diabetes-Readmission-Prediction\\src\\data_preparation\\collators.py:41\u001b[0m, in \u001b[0;36mpad_collate_fn\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m     39\u001b[0m learned_label_sequences \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch \u001b[38;5;129;01mand\u001b[39;00m batch[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m---> 41\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m col_name \u001b[38;5;129;01min\u001b[39;00m \u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfeatures\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearned_labels\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m     42\u001b[0m          learned_label_sequences[col_name] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Precomputed Embeddings (one list per configured column)\u001b[39;00m\n",
      "\u001b[1;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "# Número de ejemplos de fondo\n",
    "N_BG = 50\n",
    "bg_batches = []\n",
    "count = 0\n",
    "\n",
    "for batch in train_loader:\n",
    "    # batch: dict{'num_ohe', 'learned_labels', 'precomputed_labels', 'mask', ...}\n",
    "    # Reconstruye el campo 'features' que espera tu predictor:\n",
    "    features = {\n",
    "      'num_ohe':            batch['num_ohe'],            # (b, seq_len, d₁)\n",
    "      'learned_labels':     batch['learned_labels'],     # dict de (b, seq_len)\n",
    "      'precomputed_labels': batch['precomputed_labels'], # dict de (b, seq_len)\n",
    "    }\n",
    "    bg_batches.append({\n",
    "      'features': features,\n",
    "      'mask':     batch['mask']                         # (b, seq_len)\n",
    "    })\n",
    "    count += batch['num_ohe'].size(0)\n",
    "    if count >= N_BG:\n",
    "        break\n",
    "\n",
    "# Aplana la lista de batches a una lista de ejemplos\n",
    "# y recorta a exactamente N_BG:\n",
    "bg_list = []\n",
    "for b in bg_batches:\n",
    "    batch_size = b['mask'].size(0)\n",
    "    for i in range(batch_size):\n",
    "        # extrae el i-ésimo ejemplar de cada tensor\n",
    "        single = {\n",
    "          'features': {\n",
    "            'num_ohe':            b['features']['num_ohe'][i:i+1],\n",
    "            'learned_labels':     {col: t[i:i+1] for col,t in b['features']['learned_labels'].items()},\n",
    "            'precomputed_labels': {col: t[i:i+1] for col,t in b['features']['precomputed_labels'].items()},\n",
    "          },\n",
    "          'mask': b['mask'][i:i+1]\n",
    "        }\n",
    "        bg_list.append(single)\n",
    "        if len(bg_list) == N_BG:\n",
    "            break\n",
    "    if len(bg_list) == N_BG:\n",
    "        break\n",
    "\n",
    "# Convierte la lista a un único batch concatenado\n",
    "def collate_bg(list_of_dicts):\n",
    "    # reutiliza tu pad_collate_fn desplegando\n",
    "    #   cada dict como si fuese un batch de tamaño 1\n",
    "    return pad_collate_fn([\n",
    "      {\n",
    "        'features': {\n",
    "          **{'num_ohe': ds['features']['num_ohe'][0]},\n",
    "          **{'learned_labels': {col: ds['features']['learned_labels'][col][0] for col in ds['features']['learned_labels']}},\n",
    "          **{'precomputed_labels': {col: ds['features']['precomputed_labels'][col][0] for col in ds['features']['precomputed_labels']}}\n",
    "        },\n",
    "        'targets': torch.zeros(1, dtype=torch.long),  # dummy\n",
    "        'length': ds['mask'].shape[1],\n",
    "        'patient_id': None\n",
    "      }\n",
    "      for ds in list_of_dicts\n",
    "    ])\n",
    "\n",
    "bg_collated = collate_bg(bg_list)\n",
    "# bg_collated['features'] es el dict con tensores (50, seq_len, …)\n",
    "# bg_collated['mask']     es (50, seq_len)\n",
    "\n",
    "# Pásalo a device\n",
    "for k,v in bg_collated['features'].items():\n",
    "    if isinstance(v, torch.Tensor):\n",
    "        bg_collated['features'][k] = v.to(device)\n",
    "for col in bg_collated['features']['learned_labels']:\n",
    "    bg_collated['features']['learned_labels'][col] = bg_collated['features']['learned_labels'][col].to(device)\n",
    "for col in bg_collated['features']['precomputed_labels']:\n",
    "    bg_collated['features']['precomputed_labels'][col] = bg_collated['features']['precomputed_labels'][col].to(device)\n",
    "bg_collated['mask'] = bg_collated['mask'].to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3efcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "shap_model     = ShapModel(predictor.model).to(device)\n",
    "shap_explainer = shap.DeepExplainer(shap_model, bg_collated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75284794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Transforma df_test\n",
    "feature_seqs_test, test_target_seqs, test_pids = data_preparer.transform(df_test)\n",
    "# 2. Toma solo el primero\n",
    "dataset_test = PatientSequenceDataset(feature_seqs_test, test_target_seqs, test_pids)\n",
    "batch_first  = pad_collate_fn([dataset_test[0]])\n",
    "# 3. Llévalo a device\n",
    "for k,v in batch_first['features'].items():\n",
    "    batch_first['features'][k] = v.to(device)\n",
    "\n",
    "batch_first['mask'] = batch_first['mask'].to(device)\n",
    "\n",
    "X_batch = batch_first\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d36b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values = shap_explainer.shap_values(X_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859a0d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clase predicha\n",
    "probs_first = shap_model(X_batch).cpu().detach().numpy()  # (1, n_classes)\n",
    "pred_cls    = int(probs_first.argmax(axis=1)[0])\n",
    "\n",
    "# Valores SHAP de la clase predicha\n",
    "sv = shap_values[pred_cls][0]                             # (seq_len, n_feats)\n",
    "\n",
    "# Importancia media por característica\n",
    "mean_abs = sv.abs().mean(axis=0)                          # (n_feats,)\n",
    "feat_names = data_preparer.feature_cols\n",
    "\n",
    "for name, imp in zip(feat_names, mean_abs):\n",
    "    print(f\"{name:20s}: {imp:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf15c544",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()\n",
    "shap.force_plot(\n",
    "    shap_explainer.expected_value[pred_cls],\n",
    "    sv,\n",
    "    X_batch['features']['num_ohe'][0].cpu().numpy(),\n",
    "    feature_names=feat_names,\n",
    "    matplotlib=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df713982",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
